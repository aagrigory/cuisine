{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.json', 'train.json']\n"
     ]
    }
   ],
   "source": [
    "# To run this code you will need tensorflow package installed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bab464738c5d28378b848382db6dd4934a693a0"
   },
   "outputs": [],
   "source": [
    "list_of_files = [f for f in os.listdir(\"../input\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c9884619c098f8c85c1c96474eba1e5eb3c7796"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"../input/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90cc740b6467ee195fba9ca26a3f0c79525423f1"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_json(\"../input/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7669228bc8b44b3cb08ef202c2af53199cc04a41"
   },
   "outputs": [],
   "source": [
    "# data.shape = (39774, 3)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7fcc2b5c3ea52c34ce91d43ef3c76a637484b6c"
   },
   "outputs": [],
   "source": [
    "# adding a new column for ingredients by converting list of ingredients to ingredient strings\n",
    "data['ingredients_str'] = data['ingredients'].apply(lambda x: ' '.join([w for w in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51a5b4c7597db08b581e01fd282b9a2663d4916c"
   },
   "outputs": [],
   "source": [
    "# removing duplicate (cuisine, ingredients) combinations\n",
    "data.drop_duplicates(['cuisine', 'ingredients_str'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69a7ffb9e60e46de480d49fb2fd747c336fe8098"
   },
   "outputs": [],
   "source": [
    "# identifying rows where there are two or more different cuisines for the same ingredients\n",
    "# remove the ambiguity\n",
    "g = data.groupby('ingredients_str')\n",
    "data_to_remove = g.filter(lambda x: len(x) > 1)\n",
    "dfTemp = pd.merge(data, data_to_remove, how='left', on='id') \n",
    "data = dfTemp[dfTemp.ingredients_y.isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f2bd7773de08101d98be6719646cab7c313b6f7"
   },
   "outputs": [],
   "source": [
    "# data.shape after some cleaning = (39671, 3)\n",
    "# dropping the columns that were introduced in the process and no longer are needed\n",
    "data.drop(['ingredients_str_x', 'cuisine_y', 'ingredients_y', 'ingredients_str_y'], axis=1, inplace=True)\n",
    "data.columns = ['cuisine', 'id', 'ingredients']\n",
    "g = None\n",
    "data_to_remove = None\n",
    "dfTemp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c184bee805370231054bac393d6ca48d99f33ce"
   },
   "outputs": [],
   "source": [
    "# identifying all individual ingredients from all cuisines\n",
    "list_of_ingredients = []\n",
    "for item in list(data['ingredients']):\n",
    "    list_of_ingredients += item\n",
    "\n",
    "# removing duplicate entries\n",
    "list_of_ingredients = list(set(list_of_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22a49724d094dc60cfa7ff978d8196fff79a3d15"
   },
   "outputs": [],
   "source": [
    "# creating a new column for each ingredient, and they become an indicator (binary) variables\n",
    "for ingredient in list_of_ingredients:\n",
    "    data[ingredient] = data['ingredients'].apply(lambda x: 1 if ingredient in x else 0)\n",
    "    test_data[ingredient] = test_data['ingredients'].apply(lambda x: 1 if ingredient in x else 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84a84390d9082f000f182ab3f693fb9d92bd4b75"
   },
   "outputs": [],
   "source": [
    "# viewing some date\n",
    "# data[['cuisine', 'id', 'ingredients', 'romaine lettuce', 'ground pepper', 'eggs']][:5]\n",
    "test_data[['id', 'ingredients', 'romaine lettuce', 'ground pepper', 'eggs']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8628ad13e1ef288453899dc1b7b7079451afadb7"
   },
   "outputs": [],
   "source": [
    "# creating dummy variables from the target variable\n",
    "dfDummies = pd.get_dummies(data[\"cuisine\"])\n",
    "dfMaster = pd.concat([data, dfDummies], axis=1)\n",
    "\n",
    "# breaking the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTrain, dfTest = train_test_split(dfMaster, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0c06d0387c95ee5540b10d9a44c78f0ea44a2ec"
   },
   "outputs": [],
   "source": [
    "# creating list of litst in order to use with the tensorflow\n",
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for _, item in dfTrain.iterrows():\n",
    "    train_X.append(list(item[3:-20]))\n",
    "    train_Y.append(list(item[-20:]))\n",
    "\n",
    "for _, item in dfTest.iterrows():\n",
    "    test_X.append(list(item[3:-20]))\n",
    "    test_Y.append(list(item[-20:]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "abd6a4179412f2b387eef86970ce6829a28a583f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0360d264ac8c481ea1bd79b293e9ecfdec4a4d3"
   },
   "outputs": [],
   "source": [
    "# defining some of the hyper paramenters (these can be manipulated in order to tune the model)\n",
    "learning_rate = 0.01\n",
    "training_epochs = 60\n",
    "display_step = 10\n",
    "\n",
    "n_input = 6714\n",
    "n_hidden = 10000\n",
    "n_output = 20\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "weights = {\n",
    "    \"hidden\": tf.Variable(tf.random_normal([n_input, n_hidden])),\n",
    "    \"output\": tf.Variable(tf.random_normal([n_hidden, n_output]))\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    \"hidden\": tf.Variable(tf.random_normal([n_hidden])),\n",
    "    \"output\": tf.Variable(tf.random_normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3142242d06bfc17b3ca8e5c8fe7f5fff6c3dd213"
   },
   "outputs": [],
   "source": [
    "def model(X, weights, bias):\n",
    "    layer1 = tf.add(tf.matmul(X, weights[\"hidden\"]), bias[\"hidden\"] )\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    output_layer = tf.matmul(layer1, weights[\"output\"]) + bias[\"output\"]\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71ed258c8dab89a364684d31972b45f369aa3660"
   },
   "outputs": [],
   "source": [
    "# defining the cost function and the optimizer\n",
    "pred = model(X, weights, bias)\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# building and outputing the some stats for the predictive model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epochs in range(training_epochs):\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={X: train_X, Y: train_Y})\n",
    "        if(epochs + 1) % display_step == 0:\n",
    "            print(\" Epochs: {}, Cost: {}\".format((epochs+1), c))\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    test_result = sess.run(pred, feed_dict = {X: train_X})\n",
    "    score_result = sess.run(pred, feed_dict = {X: score_X})\n",
    "    \n",
    "    correct_pred = tf.equal( tf.argmax(test_result, 1), tf.argmax(train_Y, 1) )\n",
    "    \n",
    "    accuracy = tf.reduce_mean( tf.cast(correct_pred, \"float\") )\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({X: test_X, Y: test_Y})))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c3fb3c5a2379b669f5e5b71c13da542f0dbbbf3"
   },
   "outputs": [],
   "source": [
    "# creating a list for scoring\n",
    "score_X = []\n",
    "\n",
    "for _, item in test_data.iterrows():\n",
    "    score_X.append(list(item[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b685bffa42a7e00d7834d9e56e54af2f77e2df5b"
   },
   "outputs": [],
   "source": [
    "score_array = []\n",
    "for each_score in score_result:\n",
    "    idx = np.argmax(each_score)\n",
    "    score_array.append(idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
